{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2394c146-d67f-4ff8-860f-48fd23a88370",
   "metadata": {},
   "source": [
    "# Question 4 - 2 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82aa44-70de-4b65-a994-cce71516ae54",
   "metadata": {},
   "source": [
    "## data_utils.py\n",
    "Complete the two-layer neural network Jupyter notebook. Print out the entire notebook and\n",
    "relevant code and submit it as a pdf to gradescope. Download the CIFAR-10 dataset, as you\n",
    "did in HW #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e612c19-8294-4e26-a00e-4e08070db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "import platform\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "# def load_CIFAR10(ROOT):\n",
    "#     \"\"\" load all of cifar \"\"\"\n",
    "#     xs = []\n",
    "#     ys = []\n",
    "#     for b in range(1,6):\n",
    "#         f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "#         X, Y = load_CIFAR_batch(f)\n",
    "#         xs.append(X)\n",
    "#         ys.append(Y)\n",
    "#     Xtr = np.concatenate(xs)\n",
    "#     Ytr = np.concatenate(ys)\n",
    "#     del X, Y\n",
    "#     Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "#     return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\"Load all of CIFAR-10 using absolute paths.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \"\"\" NOTE FOR THE GRADERS: I had something going on with my join ROOT function\n",
    "        so I decided to simply manually join the file directory with a similar for loop\n",
    "        because I kept getting the same error despite having the correct directory\n",
    "        You can see I tested to see if the directory is present in the normal code\"\"\"\n",
    "    for b in range(1, 6):\n",
    "        f = f\"/Users/ctang/Desktop/ECE_C147/HW3/cifar-10-batches-py/data_batch_{b}\"\n",
    "        if not os.path.exists(f):\n",
    "            raise FileNotFoundError(f\"File not found: {f}\")\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "\n",
    "    test_file = \"/Users/ctang/Desktop/ECE_C147/HW3/cifar-10-batches-py/test_batch\"\n",
    "    if not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(f\"File not found: {test_file}\")\n",
    "\n",
    "    Xte, Yte = load_CIFAR_batch(test_file)\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n",
    "                     subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '/Users/ctang/Desktop/ECE_C147/HW3/cifar-10-batches-py/'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "    \n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "    \n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_val': X_val, 'y_val': y_val,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_tiny_imagenet(path, dtype=np.float32, subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
    "    TinyImageNet-200 have the same directory structure, so this can be used\n",
    "    to load any of them.\n",
    "    \n",
    "    Inputs:\n",
    "    - path: String giving path to the directory to load.\n",
    "    - dtype: numpy datatype used to load the data.\n",
    "    - subtract_mean: Whether to subtract the mean training image.\n",
    "    \n",
    "    Returns: A dictionary with the following entries:\n",
    "    - class_names: A list where class_names[i] is a list of strings giving the\n",
    "    WordNet names for class i in the loaded dataset.\n",
    "    - X_train: (N_tr, 3, 64, 64) array of training images\n",
    "    - y_train: (N_tr,) array of training labels\n",
    "    - X_val: (N_val, 3, 64, 64) array of validation images\n",
    "    - y_val: (N_val,) array of validation labels\n",
    "    - X_test: (N_test, 3, 64, 64) array of testing images.\n",
    "    - y_test: (N_test,) array of test labels; if test labels are not available\n",
    "    (such as in student code) then y_test will be None.\n",
    "    - mean_image: (3, 64, 64) array giving mean training image\n",
    "    \"\"\"\n",
    "    # First load wnids\n",
    "    with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "        wnids = [x.strip() for x in f]\n",
    "    \n",
    "    # Map wnids to integer labels\n",
    "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "    \n",
    "    # Use words.txt to get names for each class\n",
    "    with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "        for wnid, words in wnid_to_words.iteritems():\n",
    "          wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "    \n",
    "    # Next load training data.\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i, wnid in enumerate(wnids):\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print('loading training data for synset %d / %d' % (i + 1, len(wnids)))\n",
    "    # To figure out the filenames we need to open the boxes file\n",
    "    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "    with open(boxes_file, 'r') as f:\n",
    "      filenames = [x.split('\\t')[0] for x in f]\n",
    "    num_images = len(filenames)\n",
    "    \n",
    "    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n",
    "    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n",
    "    for j, img_file in enumerate(filenames):\n",
    "        img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "        img = imread(img_file)\n",
    "        if img.ndim == 2:\n",
    "        ## grayscale file\n",
    "            img.shape = (64, 64, 1)\n",
    "        X_train_block[j] = img.transpose(2, 0, 1)\n",
    "    X_train.append(X_train_block)\n",
    "    y_train.append(y_train_block)\n",
    "    \n",
    "    # We need to concatenate all training data\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    # Next load validation data\n",
    "    with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "        img_files = []\n",
    "        val_wnids = []\n",
    "        for line in f:\n",
    "            img_file, wnid = line.split('\\t')[:2]\n",
    "            img_files.append(img_file)\n",
    "            val_wnids.append(wnid)\n",
    "        num_val = len(img_files)\n",
    "        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "        X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n",
    "        for i, img_file in enumerate(img_files):\n",
    "          img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "          img = imread(img_file)\n",
    "          if img.ndim == 2:\n",
    "            img.shape = (64, 64, 1)\n",
    "          X_val[i] = img.transpose(2, 0, 1)\n",
    "    \n",
    "    # Next load test images\n",
    "    # Students won't have test labels, so we need to iterate over files in the\n",
    "    # images directory.\n",
    "    img_files = os.listdir(os.path.join(path, 'test', 'images'))\n",
    "    X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        img_file = os.path.join(path, 'test', 'images', img_file)\n",
    "        img = imread(img_file)\n",
    "        if img.ndim == 2:\n",
    "            img.shape = (64, 64, 1)\n",
    "        X_test[i] = img.transpose(2, 0, 1)\n",
    "    \n",
    "    y_test = None\n",
    "    y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n",
    "    if os.path.isfile(y_test_file):\n",
    "        with open(y_test_file, 'r') as f:\n",
    "          img_file_to_wnid = {}\n",
    "          for line in f:\n",
    "            line = line.split('\\t')\n",
    "            img_file_to_wnid[line[0]] = line[1]\n",
    "        y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n",
    "        y_test = np.array(y_test)\n",
    "    \n",
    "        mean_image = X_train.mean(axis=0)\n",
    "        if subtract_mean:\n",
    "            X_train -= mean_image[None]\n",
    "            X_val -= mean_image[None]\n",
    "            X_test -= mean_image[None]\n",
    "    \n",
    "        return {\n",
    "        'class_names': class_names,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'class_names': class_names,\n",
    "        'mean_image': mean_image,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_models(models_dir):\n",
    "    \"\"\"\n",
    "    Load saved models from disk. This will attempt to unpickle all files in a\n",
    "    directory; any files that give errors on unpickling (such as README.txt) will\n",
    "    be skipped.\n",
    "    \n",
    "    Inputs:\n",
    "    - models_dir: String giving the path to a directory containing model files.\n",
    "    Each model file is a pickled dictionary with a 'model' field.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary mapping model file names to models.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for model_file in os.listdir(models_dir):\n",
    "        with open(os.path.join(models_dir, model_file), 'rb') as f:\n",
    "            try:\n",
    "                models[model_file] = load_pickle(f)['model']\n",
    "            except pickle.UnpicklingError:\n",
    "                continue\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69c200-d76b-45a2-9601-ae4851afed4d",
   "metadata": {},
   "source": [
    "## neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6c0234-629e-4961-85f9-6c4355cb14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    D, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "    \n",
    "    In other words, the network has the following architecture:\n",
    "    \n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "    \n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "        \n",
    "        W1: First layer weights; has shape (H, D)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (C, H)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "        \n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        # self.params = {}\n",
    "        # self.params['W1'] = std * np.random.randn(hidden_size, input_size)\n",
    "        # self.params['b1'] = np.zeros(hidden_size)\n",
    "        # self.params['W2'] = std * np.random.randn(output_size, hidden_size)\n",
    "        # self.params['b2'] = np.zeros(output_size)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(hidden_size, input_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "        an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "        is not passed then we only return scores, and if it is passed then we\n",
    "        instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "        \n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "        \n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "        samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "        with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "        h1 = np.dot(X, W1.T) + b1\n",
    "        h1[h1 <= 0] = 0\n",
    "        h2 = np.dot(h1, W2.T) + b2\n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Calculate the output scores of the neural network.  The result\n",
    "        #   should be (N, C). As stated in the description for this class,\n",
    "        #   there should not be a ReLU layer after the second FC layer.\n",
    "        #   The output of the second FC layer is the output scores. Do not\n",
    "        #   use a for loop in your implementation.\n",
    "        # ================================================================ #\n",
    "        real_scores = h2\n",
    "        scores = h2 - np.max(h2, axis=1, keepdims=True)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        \n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            #return scores\n",
    "            return real_scores\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Calculate the loss of the neural network.  This includes the\n",
    "        #   softmax loss and the L2 regularization for W1 and W2. Store the\n",
    "        #   total loss in teh variable loss.  Multiply the regularization\n",
    "        #   loss by 0.5 (in addition to the factor reg).\n",
    "        # ================================================================ #\n",
    "        # scores is num_examples by num_classes\n",
    "        num_examples = X.shape[0]\n",
    "        correct_logprobs = -np.log(probs[range(num_examples), y])\n",
    "        data_loss = np.sum(correct_logprobs) / num_examples\n",
    "        reg_loss = 0.5*reg*(np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "        loss = data_loss + reg_loss\n",
    "        #pass\n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Implement the backward pass.  Compute the derivatives of the\n",
    "        #   weights and the biases.  Store the results in the grads\n",
    "        #   dictionary.  e.g., grads['W1'] should store the gradient for\n",
    "        #   W1, and be of the same size as W1.\n",
    "        # ================================================================ #\n",
    "        dscores = probs\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "        grads['W2'] = np.dot(dscores.T, h1) + reg*W2\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        dh1 = np.dot(dscores, W2)\n",
    "        dh1[h1<=0]=0\n",
    "        grads['W1'] = np.dot(dh1.T, X) + reg * W1\n",
    "        grads['b1'] = np.sum(dh1, axis=0)\n",
    "        \n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "          X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "        \n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        for it in np.arange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            \n",
    "            # ================================================================ #\n",
    "            # YOUR CODE HERE:\n",
    "            #   Create a minibatch by sampling batch_size samples randomly.\n",
    "            # ================================================================ #\n",
    "            #pass\n",
    "            indexes = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[indexes]\n",
    "            y_batch = y[indexes]\n",
    "            \n",
    "            \n",
    "            # ================================================================ #\n",
    "            # END YOUR CODE HERE\n",
    "            # ================================================================ #\n",
    "            \n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # ================================================================ #\n",
    "            # YOUR CODE HERE:\n",
    "            #   Perform a gradient descent step using the minibatch to update\n",
    "            #   all parameters (i.e., W1, W2, b1, and b2).\n",
    "            # ================================================================ #\n",
    "            \n",
    "            self.params['W1'] -= learning_rate*grads['W1']\n",
    "            self.params['W2'] -= learning_rate*grads['W2']\n",
    "            self.params['b1'] -= learning_rate*grads['b1']\n",
    "            self.params['b2'] -= learning_rate*grads['b2']\n",
    "            #pass\n",
    "            \n",
    "            # ================================================================ #\n",
    "            # END YOUR CODE HERE\n",
    "            # ================================================================ #\n",
    "            \n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "            \n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                \n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "        \n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "          classify.\n",
    "        \n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "          to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Predict the class given the input data.\n",
    "        # ================================================================ #\n",
    "        #pass\n",
    "        predicted_output = np.dot(np.maximum(0, np.dot(X, self.params['W1'].T)+ self.params['b1']), self.params['W2'].T) + self.params['b2']\n",
    "        y_pred = np.argmax(predicted_output, axis=1)\n",
    "        \n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c767e-b589-4e71-98f1-b14bee783b0a",
   "metadata": {},
   "source": [
    "# Question 5 FC Nets\n",
    "Complete the FC Net Jupyter notebook. Print out the entire notebook and relevant code and submit it as a pdf to gradescope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662b7f9-33b9-4aff-a5ac-6b40fad6d843",
   "metadata": {},
   "source": [
    "## layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5258bae8-d53b-4ab5-a91a-e99dfbf193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "    \n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the output of the forward pass.  Notice the dimensions\n",
    "    #   of w are D x M, which is the transpose of what we did in earlier \n",
    "    #   assignments.\n",
    "    # ================================================================ #\n",
    "    X = x.reshape((x.shape[0], -1))\n",
    "    out = np.dot(X,w) + b\n",
    "    #pass\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the gradients for the backward pass.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    # dout is N x M\n",
    "    # dx should be N x d1 x ... x dk; it relates to dout through multiplication with w, which is D x M\n",
    "    # dw should be D x M; it relates to dout through multiplication with x, which is N x D after reshaping\n",
    "    # db should be M; it is just the sum over dout examples\n",
    "    X = x.reshape((x.shape[0], -1))\n",
    "    db = np.sum(dout, axis = 0)\n",
    "    dw = np.dot(X.T, dout)\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    #pass\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "    \n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the ReLU forward pass.\n",
    "    # ================================================================ #\n",
    "    out = np.maximum(0,x)\n",
    "    #pass\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "    \n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "    \n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the ReLU backward pass\n",
    "    # ================================================================ #\n",
    "    \n",
    "    # ReLU directs linearly to those > 0\n",
    "    #pass\n",
    "    dx = dout*(x>0)\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    return dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e44ea-a84a-4a79-90b2-9ce30ef422eb",
   "metadata": {},
   "source": [
    "## fc_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0ec83-c7b0-45c1-8062-33054b868bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from .layers import *\n",
    "from .layer_utils import *\n",
    "\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
    "    softmax loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "    \n",
    "    The architecure should be affine - relu - affine - softmax.\n",
    "    \n",
    "    Note that this class does not implement gradient descent; instead, it\n",
    "    will interact with a separate Solver object that is responsible for running\n",
    "    optimization.\n",
    "    \n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=3*32*32, hidden_dims=100, num_classes=10,\n",
    "               dropout=0, weight_scale=1e-3, reg=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "        \n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dims: An integer giving the size of the hidden layer\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - dropout: Scalar between 0 and 1 giving dropout strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Initialize W1, W2, b1, and b2.  Store these as self.params['W1'], \n",
    "        #   self.params['W2'], self.params['b1'] and self.params['b2']. The\n",
    "        #   biases are initialized to zero and the weights are initialized\n",
    "        #   so that each parameter has mean 0 and standard deviation weight_scale.\n",
    "        #   The dimensions of W1 should be (input_dim, hidden_dim) and the\n",
    "        #   dimensions of W2 should be (hidden_dims, num_classes)\n",
    "        # ================================================================ #\n",
    "        self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dims))\n",
    "        self.params['W2'] = np.random.normal(0, weight_scale, (hidden_dims, num_classes))\n",
    "        self.params['b1'] = np.zeros(hidden_dims)\n",
    "        self.params['b2'] = np.zeros(num_classes)\n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "        \n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "        \n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"  \n",
    "        scores = None\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Implement the forward pass of the two-layer neural network. Store\n",
    "        #   the class scores as the variable 'scores'.  Be sure to use the layers\n",
    "        #   you prior implemented.\n",
    "        # ================================================================ #    \n",
    "        hidden, cache_hidden = affine_relu_forward(X, self.params['W1'], self.params['b1'])\n",
    "        scores, cache_scores = affine_forward(hidden, self.params['W2'], self.params['b2'])\n",
    "        #pass\n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "          return scores\n",
    "        \n",
    "        loss, grads = 0, {}\n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Implement the backward pass of the two-layer neural net.  Store\n",
    "        #   the loss as the variable 'loss' and store the gradients in the \n",
    "        #   'grads' dictionary.  For the grads dictionary, grads['W1'] holds\n",
    "        #   the gradient for W1, grads['b1'] holds the gradient for b1, etc.\n",
    "        #   i.e., grads[k] holds the gradient for self.params[k].\n",
    "        #\n",
    "        #   Add L2 regularization, where there is an added cost 0.5*self.reg*W^2\n",
    "        #   for each W.  Be sure to include the 0.5 multiplying factor to \n",
    "        #   match our implementation.\n",
    "        #\n",
    "        #   And be sure to use the layers you prior implemented.\n",
    "        # ================================================================ #    \n",
    "        loss, dout = softmax_loss(scores,y)\n",
    "        loss += 0.5 * self.reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))\n",
    "    \n",
    "        dh, dw2, db2 = affine_backward(dout, cache_scores)\n",
    "        dx, dw1, db1 = affine_relu_backward(dh, cache_hidden)\n",
    "    \n",
    "        grads['W1'] = dw1 + self.reg * self.params['W1']\n",
    "        grads['b1'] = db1\n",
    "        grads['W2'] = dw2 + self.reg * self.params['W2']\n",
    "        grads['b2'] = db2\n",
    "    \n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "\n",
    "class FullyConnectedNet(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "    ReLU nonlinearities, and a softmax loss function. This will also implement\n",
    "    dropout and batch normalization as options. For a network with L layers,\n",
    "    the architecture will be\n",
    "    \n",
    "    {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "    \n",
    "    where batch normalization and dropout are optional, and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "    \n",
    "    Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "    self.params dictionary and will be learned using the Solver class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "               dropout=0, use_batchnorm=False, reg=0.0,\n",
    "               weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize a new FullyConnectedNet.\n",
    "        \n",
    "        Inputs:\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_classes: An integer giving the number of classes to classify.\n",
    "        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n",
    "          the network should not use dropout at all.\n",
    "        - use_batchnorm: Whether or not the network should use batch normalization.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "          this datatype. float32 is faster but less accurate, so you should use\n",
    "          float64 for numeric gradient checking.\n",
    "        - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "          will make the dropout layers deteriminstic so we can gradient check the\n",
    "          model.\n",
    "        \"\"\"\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = dropout > 0\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Initialize all parameters of the network in the self.params dictionary.\n",
    "        #   The weights and biases of layer 1 are W1 and b1; and in general the \n",
    "        #   weights and biases of layer i are Wi and bi. The\n",
    "        #   biases are initialized to zero and the weights are initialized\n",
    "        #   so that each parameter has mean 0 and standard deviation weight_scale.\n",
    "        # ================================================================ #\n",
    "        for i in np.arange(self.num_layers):\n",
    "            if(i == 0):\n",
    "                self.params['W' + str(i+1)] = np.random.normal(0, weight_scale, (input_dim, hidden_dims[i]))\n",
    "                self.params['b' + str(i+1)] = np.zeros(hidden_dims[i])\n",
    "            elif(i == self.num_layers - 1):\n",
    "                self.params['W' + str(i+1)] = np.random.normal(0, weight_scale, (hidden_dims[i-1], num_classes))\n",
    "                self.params['b' + str(i+1)] = np.zeros(num_classes)\n",
    "            else:\n",
    "                self.params['W' + str(i+1)] = np.random.normal(0, weight_scale, (hidden_dims[i-1], hidden_dims[i]))\n",
    "                self.params['b' + str(i+1)] = np.zeros(hidden_dims[i])\n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        # When using dropout we need to pass a dropout_param dictionary to each\n",
    "        # dropout layer so that the layer knows the dropout probability and the mode\n",
    "        # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "        self.dropout_param = {}\n",
    "        if self.use_dropout:\n",
    "          self.dropout_param = {'mode': 'train', 'p': dropout}\n",
    "          if seed is not None:\n",
    "            self.dropout_param['seed'] = seed\n",
    "        \n",
    "        # With batch normalization we need to keep track of running means and\n",
    "        # variances, so we need to pass a special bn_param object to each batch\n",
    "        # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "        # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "        # pass of the second batch normalization layer, etc.\n",
    "        self.bn_params = []\n",
    "        if self.use_batchnorm:\n",
    "          self.bn_params = [{'mode': 'train'} for i in np.arange(self.num_layers - 1)]\n",
    "        \n",
    "        # Cast all parameters to the correct datatype\n",
    "        for k, v in self.params.items():\n",
    "          self.params[k] = v.astype(dtype)\n",
    "    \n",
    "    \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for the fully-connected net.\n",
    "        \n",
    "        Input / output: Same as TwoLayerNet above.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        \n",
    "        # Set train/test mode for batchnorm params and dropout param since they\n",
    "        # behave differently during training and testing.\n",
    "        if self.dropout_param is not None:\n",
    "          self.dropout_param['mode'] = mode   \n",
    "        if self.use_batchnorm:\n",
    "          for bn_param in self.bn_params:\n",
    "            bn_param[mode] = mode\n",
    "        \n",
    "        scores = None\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Implement the forward pass of the FC net and store the output\n",
    "        #   scores as the variable \"scores\".\n",
    "        # ================================================================ #\n",
    "        H = []\n",
    "        H_cache = []\n",
    "        for i in range(self.num_layers):\n",
    "            H_app = None\n",
    "            H_cache_app = None\n",
    "            if(i==0):\n",
    "                H_app, H_cache_app = affine_relu_forward(X, self.params['W' + str(i+1)], self.params['b' + str(i+1)])\n",
    "                H.append(H_app)\n",
    "                H_cache.append(H_cache_app)\n",
    "            elif(i == self.num_layers - 1):\n",
    "                scores, H_cache_app = affine_forward(H[i-1], self.params['W' + str(i+1)], self.params['b' + str(i+1)])\n",
    "                H_cache.append(H_cache_app)\n",
    "            else:\n",
    "                H_app, H_cache_app = affine_relu_forward(H[i-1], self.params['W' + str(i+1)], self.params['b' + str(i+1)])\n",
    "                H.append(H_app)\n",
    "                H_cache.append(H_cache_app)\n",
    "          \n",
    "        #pass\n",
    "        \n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        \n",
    "        # If test mode return early\n",
    "        if mode == 'test':\n",
    "          return scores\n",
    "        \n",
    "        loss, grads = 0.0, {}\n",
    "        # ================================================================ #\n",
    "        # YOUR CODE HERE:\n",
    "        #   Implement the backwards pass of the FC net and store the gradients\n",
    "        #   in the grads dict, so that grads[k] is the gradient of self.params[k]\n",
    "        #   Be sure your L2 regularization includes a 0.5 factor.\n",
    "        # ================================================================ #\n",
    "        \n",
    "        #pass\n",
    "        loss, dhidden = softmax_loss(scores, y)\n",
    "        for i in range(self.num_layers,0,-1):\n",
    "          loss += 0.5*self.reg*np.sum(self.params['W{}'.format(i)]*self.params['W{}'.format(i)])\n",
    "          if i == self.num_layers:\n",
    "            dH1, dW, db = affine_backward(dhidden,H_cache[i-1])\n",
    "            grads['W{}'.format(i)] = dW + self.reg*self.params['W{}'.format(i)]\n",
    "            grads['b{}'.format(i)] = db\n",
    "          else:\n",
    "            dH1, dW, db = affine_relu_backward(dH1,H_cache[i-1])\n",
    "            grads['W{}'.format(i)] = dW + self.reg*self.params['W{}'.format(i)]\n",
    "            grads['b{}'.format(i)] = db\n",
    "        # loss, dhidden = softmax_loss(scores, y)\n",
    "        # for i in range(self.num_layers,0,-1):\n",
    "        #     loss += 0.5 * self.reg*np.sum(self.params['W{}'.format(i)]*self.params['W{}'.format(i)])\n",
    "        #     if i == self.num_layers:\n",
    "        #         dFC1, dW, db = affine_backward(dhidden,FC_cache[i-1])\n",
    "        #         grads['W{}'.format(i)] = dW + self.reg*self.params['W{}'.format(i)]\n",
    "        #         grads['b{}'.format(i)] = db\n",
    "        #     else:\n",
    "        #       dFC1, dW, db = affine_relu_backward(dFC1,FC_cache[i-1])\n",
    "        #         grads['W{}'.format(i)] = dW + self.reg*self.params['W{}'.format(i)]\n",
    "        #         grads['b{}'.format(i)] = db\n",
    "        # ================================================================ #\n",
    "        # END YOUR CODE HERE\n",
    "        # ================================================================ #\n",
    "        return loss, grads\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
